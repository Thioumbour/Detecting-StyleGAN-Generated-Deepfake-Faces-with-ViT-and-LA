{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 924245,
          "sourceType": "datasetVersion",
          "datasetId": 464091
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Xception",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "dagnelies_deepfake_faces_path = kagglehub.dataset_download('dagnelies/deepfake-faces')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "4PDCEd_MmC1D"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T05:33:01.798653Z",
          "iopub.execute_input": "2025-02-18T05:33:01.799319Z",
          "iopub.status.idle": "2025-02-18T05:33:08.387212Z",
          "shell.execute_reply.started": "2025-02-18T05:33:01.799264Z",
          "shell.execute_reply": "2025-02-18T05:33:08.385634Z"
        },
        "id": "G3L6HK4JmC1F",
        "outputId": "465f7649-85cf-46d3-d1de-f09f3149a640"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting torchviz\n  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.5.1+cu121)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (3.0.2)\nDownloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\nInstalling collected packages: torchviz\nSuccessfully installed torchviz-0.0.3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 1. Imports et Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import ViTForImageClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, matthews_corrcoef\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, log_loss, recall_score, precision_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Classes et Fonctions de Base\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Dataset personnalisé pour les images de deepfake\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        # Conversion BGR -> RGB car OpenCV charge en BGR mais PyTorch attend RGB\n",
        "        self.images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images]\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Transformations de données\n",
        "def get_transforms():\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "    return transform\n",
        "\n",
        "# Charger le modèle ViT pré-entraîné\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    num_labels=2,  # 2 classes : fake et real\n",
        "    ignore_mismatched_sizes=True  # Ignorer les incompatibilités de taille\n",
        ")\n",
        "\n",
        "# Charger le feature extractor (pour le prétraitement des images)\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")# -----------------------------------------------------------------------------\n",
        "# 3. Chargement des Données\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Créer les dataloaders\n",
        "transform = get_transforms()\n",
        "train_dataset = DeepfakeDataset(X_train, y_train, transform=transform)\n",
        "val_dataset = DeepfakeDataset(X_val, y_val, transform=transform)\n",
        "test_dataset = DeepfakeDataset(X_test, y_test, transform=transform)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Entraînement du Modèle\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Configurer l'utilisation du GPU/CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instancier le modèle\n",
        "#model = CustomViTWithLatentAttention(num_classes=2, num_latents=64)\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimizer et critère de perte\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4, weight_decay=0.01)\n",
        "criterion = CrossEntropyLoss()\n",
        "scaler = torch.amp.GradScaler()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "\n",
        "# Dossier pour sauvegarder les résultats\n",
        "results_dir = f\"deepfake_results_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Boucle d'entraînement\n",
        "epochs = 15\n",
        "patience = 5\n",
        "best_valid_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    model.train()\n",
        "    running_loss, running_accuracy = 0.0, 0.0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda' if device == 'cuda' else 'cpu'):\n",
        "            outputs = model(images)  # outputs est un objet ImageClassifierOutput\n",
        "            logits = outputs.logits  # Extraire les logits\n",
        "            loss = criterion(logits, labels)  # Utiliser les logits pour calculer la perte\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)  # Utiliser les logits pour les prédictions\n",
        "        running_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = running_accuracy / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    valid_loss, valid_accuracy = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Valid]\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)  # outputs est un objet ImageClassifierOutput\n",
        "            logits = outputs.logits  # Extraire les logits\n",
        "            loss = criterion(logits, labels)  # Utiliser les logits pour calculer la perte\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)  # Utiliser les logits pour les prédictions\n",
        "            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "    valid_loss = valid_loss / len(val_loader)\n",
        "    valid_accuracy = valid_accuracy / len(val_loader)\n",
        "\n",
        "    # Early stopping\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), os.path.join(results_dir, 'best_model.pth'))\n",
        "        print(f\"Model saved with improved validation loss: {valid_loss:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    scheduler.step(valid_loss)\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {epoch_time:.2f}s\")\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}\")\n",
        "    print(f\"Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "        break\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T23:34:06.241036Z",
          "iopub.execute_input": "2025-02-27T23:34:06.241421Z",
          "iopub.status.idle": "2025-02-28T01:25:30.561156Z",
          "shell.execute_reply.started": "2025-02-27T23:34:06.241389Z",
          "shell.execute_reply": "2025-02-28T01:25:30.560128Z"
        },
        "id": "Y5IyVCbhmC1G",
        "outputId": "13c468f9-6fbb-4cb6-8c67-af82c370843f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1/15 [Train]: 100%|██████████| 570/570 [10:46<00:00,  1.13s/it]\nEpoch 1/15 [Valid]: 100%|██████████| 245/245 [01:33<00:00,  2.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model saved with improved validation loss: 0.5828\nEpoch 1/15 - Time: 741.28s\nTrain Loss: 0.6628, Train Accuracy: 0.6041\nValid Loss: 0.5828, Valid Accuracy: 0.6806\n--------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/15 [Train]: 100%|██████████| 570/570 [10:47<00:00,  1.14s/it]\nEpoch 2/15 [Valid]: 100%|██████████| 245/245 [01:34<00:00,  2.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model saved with improved validation loss: 0.4210\nEpoch 2/15 - Time: 742.78s\nTrain Loss: 0.5216, Train Accuracy: 0.7357\nValid Loss: 0.4210, Valid Accuracy: 0.7987\n--------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/15 [Train]: 100%|██████████| 570/570 [10:48<00:00,  1.14s/it]\nEpoch 3/15 [Valid]: 100%|██████████| 245/245 [01:34<00:00,  2.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/15 - Time: 742.84s\nTrain Loss: 0.3791, Train Accuracy: 0.8234\nValid Loss: 0.4502, Valid Accuracy: 0.7880\n--------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/15 [Train]: 100%|██████████| 570/570 [10:48<00:00,  1.14s/it]\nEpoch 4/15 [Valid]: 100%|██████████| 245/245 [01:34<00:00,  2.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model saved with improved validation loss: 0.3313\nEpoch 4/15 - Time: 743.60s\nTrain Loss: 0.2565, Train Accuracy: 0.8876\nValid Loss: 0.3313, Valid Accuracy: 0.8663\n--------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/15 [Train]: 100%|██████████| 570/570 [10:48<00:00,  1.14s/it]\nEpoch 5/15 [Valid]: 100%|██████████| 245/245 [01:34<00:00,  2.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5/15 - Time: 742.84s\nTrain Loss: 0.1514, Train Accuracy: 0.9388\nValid Loss: 0.4945, Valid Accuracy: 0.8422\n--------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/15 [Train]: 100%|██████████| 570/570 [10:46<00:00,  1.13s/it]\nEpoch 6/15 [Valid]: 100%|██████████| 245/245 [01:34<00:00,  2.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6/15 - Time: 740.69s\nTrain Loss: 0.1212, Train Accuracy: 0.9524\nValid Loss: 0.4310, Valid Accuracy: 0.8679\n--------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/15 [Train]: 100%|██████████| 570/570 [10:47<00:00,  1.14s/it]\nEpoch 7/15 [Valid]: 100%|██████████| 245/245 [01:34<00:00,  2.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7/15 - Time: 741.66s\nTrain Loss: 0.1028, Train Accuracy: 0.9616\nValid Loss: 0.6425, Valid Accuracy: 0.8540\n--------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/15 [Train]: 100%|██████████| 570/570 [10:47<00:00,  1.14s/it]\nEpoch 8/15 [Valid]: 100%|██████████| 245/245 [01:34<00:00,  2.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8/15 - Time: 741.36s\nTrain Loss: 0.0823, Train Accuracy: 0.9798\nValid Loss: 1.0932, Valid Accuracy: 0.8676\n--------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/15 [Train]: 100%|██████████| 570/570 [10:49<00:00,  1.14s/it]\nEpoch 9/15 [Valid]: 100%|██████████| 245/245 [01:34<00:00,  2.60it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9/15 - Time: 743.45s\nTrain Loss: 0.0337, Train Accuracy: 0.9931\nValid Loss: 1.3341, Valid Accuracy: 0.8846\n--------------------------------------------------\nEarly stopping triggered after 9 epochs\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 5. Évaluation du Modèle\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Charger le meilleur modèle\n",
        "model.load_state_dict(torch.load(os.path.join(results_dir, 'best_model.pth')))\n",
        "\n",
        "# Évaluation sur l'ensemble de test\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "test_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)  # outputs est un objet ImageClassifierOutput\n",
        "        logits = outputs.logits  # Extraire les logits\n",
        "        loss = criterion(logits, labels)  # Utiliser les logits pour calculer la perte\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        probs = torch.softmax(logits, dim=1)  # Calculer les probabilités à partir des logits\n",
        "        _, preds = torch.max(probs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# Convertir all_probs en un tableau NumPy 2D\n",
        "all_probs_np = np.array(all_probs)\n",
        "\n",
        "# Calcul des métriques\n",
        "test_loss = test_loss / len(test_loader)\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "roc_auc_value = roc_auc_score(all_labels, all_probs_np[:, 1])\n",
        "mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "kappa = cohen_kappa_score(all_labels, all_preds)\n",
        "logloss = log_loss(all_labels, all_probs)\n",
        "recall = recall_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds)\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# Affichage des métriques\n",
        "print(\"\\nTest Metrics:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_value:.4f}\")\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")\n",
        "print(f\"Kappa: {kappa:.4f}\")\n",
        "print(f\"Log Loss: {logloss:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Sauvegarder les métriques\n",
        "metrics_df = pd.DataFrame({\n",
        "    'test_loss': [test_loss],\n",
        "    'accuracy': [accuracy],\n",
        "    'roc_auc': [roc_auc_value],\n",
        "    'mcc': [mcc],\n",
        "    'f1': [f1],\n",
        "    'specificity': [specificity],\n",
        "    'kappa': [kappa],\n",
        "    'logloss': [logloss],\n",
        "    'recall': [recall],\n",
        "    'precision': [precision]\n",
        "})\n",
        "metrics_df.to_csv(os.path.join(results_dir, 'test_metrics.csv'), index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T01:47:08.425051Z",
          "iopub.execute_input": "2025-02-28T01:47:08.425402Z",
          "iopub.status.idle": "2025-02-28T01:48:27.925065Z",
          "shell.execute_reply.started": "2025-02-28T01:47:08.425371Z",
          "shell.execute_reply": "2025-02-28T01:48:27.924059Z"
        },
        "id": "snDXgyfCmC1I",
        "outputId": "20dd1ef4-e45a-461c-fc85-e0772bef3598"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Evaluating: 100%|██████████| 204/204 [01:19<00:00,  2.58it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTest Metrics:\nTest Loss: 0.3443\nAccuracy: 0.8635\nROC-AUC: 0.9432\nMCC: 0.7295\nF1 Score: 0.8690\nSpecificity: 0.8211\nKappa: 0.7269\nLog Loss: 0.3446\nRecall: 0.9058\nPrecision: 0.8351\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir all_probs en un tableau NumPy 2D\n",
        "all_probs_np = np.array(all_probs)\n",
        "\n",
        "# Calculer ROC-AUC\n",
        "roc_auc_value = roc_auc_score(all_labels, all_probs_np[:, 1])  # Accéder à la deuxième colonne\n",
        "print(f\"ROC-AUC: {roc_auc_value:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T01:43:20.541396Z",
          "iopub.execute_input": "2025-02-28T01:43:20.541766Z",
          "iopub.status.idle": "2025-02-28T01:43:20.559699Z",
          "shell.execute_reply.started": "2025-02-28T01:43:20.541737Z",
          "shell.execute_reply": "2025-02-28T01:43:20.558505Z"
        },
        "id": "5D3WfA4hmC1J",
        "outputId": "f4a5d782-5895-4858-dcfa-59f913aebcfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ROC-AUC: 0.9432\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}