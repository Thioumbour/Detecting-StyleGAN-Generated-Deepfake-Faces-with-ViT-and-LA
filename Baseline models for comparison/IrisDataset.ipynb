{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V28",
      "name": "IrisDataset"
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 939937,
          "sourceType": "datasetVersion",
          "datasetId": 501529
        },
        {
          "sourceId": 9899432,
          "sourceType": "datasetVersion",
          "datasetId": 6080940
        },
        {
          "sourceId": 10102015,
          "sourceType": "datasetVersion",
          "datasetId": 6230878
        },
        {
          "sourceId": 10119502,
          "sourceType": "datasetVersion",
          "datasetId": 6243942
        },
        {
          "sourceId": 10124102,
          "sourceType": "datasetVersion",
          "datasetId": 6247412
        },
        {
          "sourceId": 10631394,
          "sourceType": "datasetVersion",
          "datasetId": 6582382
        }
      ],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "AtFc6HLQm3tU"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "xhlulu_140k_real_and_fake_faces_path = kagglehub.dataset_download('xhlulu/140k-real-and-fake-faces')\n",
        "sokhnaballytour_test_stylegan_path = kagglehub.dataset_download('sokhnaballytour/test-stylegan')\n",
        "sokhnaballytour_test_progan_path = kagglehub.dataset_download('sokhnaballytour/test-progan')\n",
        "sokhnaballytour_val_dataset_path = kagglehub.dataset_download('sokhnaballytour/val-dataset')\n",
        "sokhnaballytour_train_dataset_path = kagglehub.dataset_download('sokhnaballytour/train-dataset')\n",
        "sokhnaballytour_data_iris_path = kagglehub.dataset_download('sokhnaballytour/data-iris')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "jI04HVrtm3tW"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision import transforms\n",
        "\n",
        "class AttentionModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionModule, self).__init__()\n",
        "        self.trunk_branch = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels)\n",
        "        )\n",
        "        self.soft_mask_branch = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // 2, kernel_size=1),\n",
        "            nn.BatchNorm2d(in_channels // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels // 2, in_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        trunk = self.trunk_branch(x)\n",
        "        mask = self.soft_mask_branch(x)\n",
        "        return trunk * (1 + mask)\n",
        "\n",
        "class ResidualAttentionNetwork(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(ResidualAttentionNetwork, self).__init__()\n",
        "        # Couche d'entrée\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Modules d'attention\n",
        "        self.attention1 = AttentionModule(16)  # 16 canaux en entrée\n",
        "        self.attention2 = AttentionModule(32)  # 32 canaux en entrée\n",
        "        self.attention3 = AttentionModule(64)  # 64 canaux en entrée\n",
        "\n",
        "        # Couches supplémentaires pour gérer les dimensions\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Couche de sortie\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.attention1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.attention2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.attention3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "\n",
        "def wmw_auc_loss(y_pred, y_true, gamma=0.4, p=2):\n",
        "    positive = y_pred[y_true == 1]\n",
        "    negative = y_pred[y_true == 0]\n",
        "    if len(positive) == 0 or len(negative) == 0:\n",
        "        return torch.tensor(0.0, device=y_pred.device)\n",
        "    diff = positive.unsqueeze(1) - negative.unsqueeze(0)\n",
        "    loss = torch.where(diff < gamma, (-(diff - gamma)) ** p, torch.tensor(0.0, device=y_pred.device))\n",
        "    return loss.mean()\n",
        "\n",
        "def combined_loss(y_pred, y_true, alpha=0.4):\n",
        "    bce_loss = nn.BCELoss()(y_pred, y_true)\n",
        "    auc_loss = wmw_auc_loss(y_pred, y_true)\n",
        "    return alpha * bce_loss + (1 - alpha) * auc_loss\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n"
      ],
      "metadata": {
        "id": "erNcJ8eGhzOw",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T00:50:07.46559Z",
          "iopub.execute_input": "2025-02-05T00:50:07.465976Z",
          "iopub.status.idle": "2025-02-05T00:50:07.484627Z",
          "shell.execute_reply.started": "2025-02-05T00:50:07.465948Z",
          "shell.execute_reply": "2025-02-05T00:50:07.483397Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import roc_curve\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from torchmetrics import CohenKappa\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    matthews_corrcoef,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    cohen_kappa_score,\n",
        "    log_loss,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        ")\n",
        "\n",
        "# Charger le modèle sur CPU ou GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Hyperparamètres\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 20\n",
        "alpha = 0.4\n",
        "\n",
        "class IrisDataset(Dataset):\n",
        "    def __init__(self, real_iris_dir, fake_iris_dir, transform=None):\n",
        "        self.real_images = [os.path.join(real_iris_dir, img) for img in os.listdir(real_iris_dir)]\n",
        "        self.fake_images = [os.path.join(fake_iris_dir, img) for img in os.listdir(fake_iris_dir)]\n",
        "        self.all_images = self.real_images + self.fake_images\n",
        "        self.labels = [0] * len(self.real_images) + [1] * len(self.fake_images)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.all_images[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "def evaluate_model(model, data_loader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.sigmoid(outputs).squeeze()\n",
        "            preds = (probs >= threshold).float()\n",
        "\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    return np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
        "\n",
        "def find_optimal_threshold(labels, probs):\n",
        "    fpr, tpr, thresholds = roc_curve(labels, probs)\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    return thresholds[optimal_idx]\n",
        "\n",
        "# Définir les transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Charger les datasets\n",
        "train_dataset = IrisDataset(real_iris_dir=\"/kaggle/input/train-dataset/train_dataset/0_real\",\n",
        "                           fake_iris_dir=\"/kaggle/input/train-dataset/train_dataset/1_fake\",\n",
        "                           transform=transform)\n",
        "val_dataset = IrisDataset(real_iris_dir=\"/kaggle/input/val-dataset/val_dataset/0_real\",\n",
        "                         fake_iris_dir=\"/kaggle/input/val-dataset/val_dataset/1_fake\",\n",
        "                         transform=transform)\n",
        "test_dataset = IrisDataset(real_iris_dir=\"/kaggle/input/test-progan/progan/person/0_real\",\n",
        "                          fake_iris_dir=\"/kaggle/input/test-progan/progan/person/1_fake\",\n",
        "                          transform=transform)\n",
        "\n",
        "# Calculer les poids des classes\n",
        "class_weights = compute_class_weight('balanced',\n",
        "                                   classes=np.unique(train_dataset.labels),\n",
        "                                   y=train_dataset.labels)\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "# Charger les données\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResidualAttentionNetwork().to(device)\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Modifier l'optimiseur pour inclure la régularisation L2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# Critère avec poids des classes\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "\n",
        "# Pour stocker les métriques\n",
        "metrics_history = []\n",
        "\n",
        "# Boucle d'entraînement\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.squeeze(-1), labels.float())\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Évaluation sur l'ensemble de validation\n",
        "    val_labels, val_preds, val_probs = evaluate_model(model, val_loader, device)\n",
        "\n",
        "    # Trouver le seuil optimal\n",
        "    optimal_threshold = find_optimal_threshold(val_labels, val_probs)\n",
        "\n",
        "    # Réévaluer avec le seuil optimal\n",
        "    val_preds = (val_probs >= optimal_threshold).astype(float)\n",
        "\n",
        "    # Calculer et sauvegarder les métriques\n",
        "    epoch_metrics = {\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss/len(train_loader),\n",
        "        'optimal_threshold': optimal_threshold,\n",
        "        'accuracy': accuracy_score(val_labels, val_preds),\n",
        "        'auc': roc_auc_score(val_labels, val_probs),\n",
        "        'precision': precision_score(val_labels, val_preds),\n",
        "        'recall': recall_score(val_labels, val_preds),\n",
        "        'f1': f1_score(val_labels, val_preds),\n",
        "        'mcc': matthews_corrcoef(val_labels, val_preds)\n",
        "    }\n",
        "\n",
        "    metrics_history.append(epoch_metrics)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "    print(f\"Training Loss: {epoch_metrics['train_loss']:.4f}\")\n",
        "    print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
        "    print(f\"Validation Metrics:\")\n",
        "    print(f\"Accuracy: {epoch_metrics['accuracy']:.4f}\")\n",
        "    print(f\"AUC: {epoch_metrics['auc']:.4f}\")\n",
        "    print(f\"F1: {epoch_metrics['f1']:.4f}\")\n",
        "\n",
        "    # Sauvegarder les métriques dans un CSV\n",
        "    pd.DataFrame(metrics_history).to_csv('training_metrics.csv', index=False)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Évaluation finale sur l'ensemble de test\n",
        "test_labels, test_preds, test_probs = evaluate_model(model, test_loader, device, threshold=optimal_threshold)\n",
        "final_metrics = {\n",
        "    'accuracy': accuracy_score(test_labels, test_preds),\n",
        "    'auc': roc_auc_score(test_labels, test_probs),\n",
        "    'precision': precision_score(test_labels, test_preds),\n",
        "    'recall': recall_score(test_labels, test_preds),\n",
        "    'f1': f1_score(test_labels, test_preds),\n",
        "    'mcc': matthews_corrcoef(test_labels, test_preds)\n",
        "}\n",
        "\n",
        "print(\"\\nMétriques finales sur l'ensemble de test:\")\n",
        "for metric, value in final_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Sauvegarder les métriques finales\n",
        "pd.DataFrame([final_metrics]).to_csv('final_test_metrics.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3LMsRIJh2ma",
        "outputId": "49f444d3-c7fe-409e-e236-099decaead0d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T00:50:12.559398Z",
          "iopub.execute_input": "2025-02-05T00:50:12.559782Z",
          "iopub.status.idle": "2025-02-05T02:42:33.454262Z",
          "shell.execute_reply.started": "2025-02-05T00:50:12.559737Z",
          "shell.execute_reply": "2025-02-05T02:42:33.453201Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1/20: 100%|██████████| 1407/1407 [10:46<00:00,  2.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 1\nTraining Loss: 0.6926\nOptimal threshold: 0.5161\nValidation Metrics:\nAccuracy: 0.5915\nAUC: 0.6125\nF1: 0.5909\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/20: 100%|██████████| 1407/1407 [05:50<00:00,  4.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 2\nTraining Loss: 0.6839\nOptimal threshold: 0.5072\nValidation Metrics:\nAccuracy: 0.5920\nAUC: 0.6235\nF1: 0.5772\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/20: 100%|██████████| 1407/1407 [05:27<00:00,  4.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 3\nTraining Loss: 0.6765\nOptimal threshold: 0.5213\nValidation Metrics:\nAccuracy: 0.6120\nAUC: 0.6530\nF1: 0.5274\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/20: 100%|██████████| 1407/1407 [05:15<00:00,  4.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 4\nTraining Loss: 0.6681\nOptimal threshold: 0.5148\nValidation Metrics:\nAccuracy: 0.6420\nAUC: 0.6780\nF1: 0.6175\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/20: 100%|██████████| 1407/1407 [05:13<00:00,  4.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 5\nTraining Loss: 0.6607\nOptimal threshold: 0.5342\nValidation Metrics:\nAccuracy: 0.6815\nAUC: 0.7439\nF1: 0.6826\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/20: 100%|██████████| 1407/1407 [05:11<00:00,  4.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 6\nTraining Loss: 0.6549\nOptimal threshold: 0.5217\nValidation Metrics:\nAccuracy: 0.6960\nAUC: 0.7586\nF1: 0.6800\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/20: 100%|██████████| 1407/1407 [05:13<00:00,  4.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 7\nTraining Loss: 0.6502\nOptimal threshold: 0.5383\nValidation Metrics:\nAccuracy: 0.6765\nAUC: 0.7342\nF1: 0.6416\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/20: 100%|██████████| 1407/1407 [05:13<00:00,  4.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 8\nTraining Loss: 0.6467\nOptimal threshold: 0.5257\nValidation Metrics:\nAccuracy: 0.7085\nAUC: 0.7719\nF1: 0.7282\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/20: 100%|██████████| 1407/1407 [05:13<00:00,  4.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 9\nTraining Loss: 0.6422\nOptimal threshold: 0.5047\nValidation Metrics:\nAccuracy: 0.6605\nAUC: 0.7007\nF1: 0.6520\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/20: 100%|██████████| 1407/1407 [05:15<00:00,  4.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 10\nTraining Loss: 0.6382\nOptimal threshold: 0.5121\nValidation Metrics:\nAccuracy: 0.7205\nAUC: 0.7926\nF1: 0.7201\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 11/20: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 11\nTraining Loss: 0.6320\nOptimal threshold: 0.5923\nValidation Metrics:\nAccuracy: 0.7370\nAUC: 0.8085\nF1: 0.7454\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 12/20: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 12\nTraining Loss: 0.6294\nOptimal threshold: 0.5284\nValidation Metrics:\nAccuracy: 0.7710\nAUC: 0.8448\nF1: 0.7744\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 13/20: 100%|██████████| 1407/1407 [05:12<00:00,  4.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 13\nTraining Loss: 0.6261\nOptimal threshold: 0.5305\nValidation Metrics:\nAccuracy: 0.7705\nAUC: 0.8484\nF1: 0.7784\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 14/20: 100%|██████████| 1407/1407 [05:15<00:00,  4.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 14\nTraining Loss: 0.6222\nOptimal threshold: 0.5073\nValidation Metrics:\nAccuracy: 0.7505\nAUC: 0.8299\nF1: 0.7600\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 15/20: 100%|██████████| 1407/1407 [05:12<00:00,  4.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 15\nTraining Loss: 0.6182\nOptimal threshold: 0.5596\nValidation Metrics:\nAccuracy: 0.7830\nAUC: 0.8601\nF1: 0.7858\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 16/20: 100%|██████████| 1407/1407 [05:11<00:00,  4.52it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 16\nTraining Loss: 0.6146\nOptimal threshold: 0.5147\nValidation Metrics:\nAccuracy: 0.7865\nAUC: 0.8617\nF1: 0.7975\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 17/20: 100%|██████████| 1407/1407 [05:13<00:00,  4.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 17\nTraining Loss: 0.6096\nOptimal threshold: 0.5104\nValidation Metrics:\nAccuracy: 0.8055\nAUC: 0.8787\nF1: 0.8164\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 18/20: 100%|██████████| 1407/1407 [05:12<00:00,  4.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 18\nTraining Loss: 0.6049\nOptimal threshold: 0.5147\nValidation Metrics:\nAccuracy: 0.8080\nAUC: 0.8858\nF1: 0.8116\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 19/20: 100%|██████████| 1407/1407 [05:15<00:00,  4.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 19\nTraining Loss: 0.6022\nOptimal threshold: 0.6001\nValidation Metrics:\nAccuracy: 0.8120\nAUC: 0.8900\nF1: 0.8103\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 20/20: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nEpoch 20\nTraining Loss: 0.5977\nOptimal threshold: 0.6078\nValidation Metrics:\nAccuracy: 0.7795\nAUC: 0.8611\nF1: 0.7690\n\nMétriques finales sur l'ensemble de test:\naccuracy: 0.7550\nauc: 0.8543\nprecision: 0.8148\nrecall: 0.6600\nf1: 0.7293\nmcc: 0.5195\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    total_log_loss = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.sigmoid(outputs).squeeze()\n",
        "            preds = (probs >= threshold).float()\n",
        "\n",
        "            # Calculer log loss\n",
        "            epsilon = 1e-7\n",
        "            probs_clipped = torch.clamp(probs, epsilon, 1 - epsilon)\n",
        "            batch_log_loss = -torch.mean(\n",
        "                labels.float() * torch.log(probs_clipped) +\n",
        "                (1 - labels.float()) * torch.log(1 - probs_clipped)\n",
        "            )\n",
        "\n",
        "            total_log_loss += batch_log_loss.item() * len(labels)\n",
        "            n_samples += len(labels)\n",
        "\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_labels), np.array(all_preds), np.array(all_probs), total_log_loss / n_samples\n",
        "\n",
        "def calculate_specificity(y_true, y_pred):\n",
        "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
        "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "\n",
        "# À la fin, pour l'évaluation finale, remplacer par:\n",
        "# Évaluation finale sur l'ensemble de test\n",
        "test_labels, test_preds, test_probs, test_log_loss = evaluate_model(\n",
        "    model, test_loader, device, threshold=optimal_threshold\n",
        ")\n",
        "final_metrics = {\n",
        "    'accuracy': accuracy_score(test_labels, test_preds),\n",
        "    'auc': roc_auc_score(test_labels, test_probs),\n",
        "    'precision': precision_score(test_labels, test_preds),\n",
        "    'recall': recall_score(test_labels, test_preds),\n",
        "    'specificity': calculate_specificity(test_labels, test_preds),\n",
        "    'f1': f1_score(test_labels, test_preds),\n",
        "    'mcc': matthews_corrcoef(test_labels, test_preds),\n",
        "    'kappa': cohen_kappa_score(test_labels, test_preds),\n",
        "    'log_loss': test_log_loss\n",
        "}\n",
        "\n",
        "print(\"\\nMétriques finales sur l'ensemble de test:\")\n",
        "for metric, value in final_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Sauvegarder les métriques finales\n",
        "pd.DataFrame([final_metrics]).to_csv('final_test_metrics.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-05T02:48:53.772413Z",
          "iopub.execute_input": "2025-02-05T02:48:53.772759Z",
          "iopub.status.idle": "2025-02-05T02:48:54.832666Z",
          "shell.execute_reply.started": "2025-02-05T02:48:53.77273Z",
          "shell.execute_reply": "2025-02-05T02:48:54.831748Z"
        },
        "id": "kdvmBYTAm3tf",
        "outputId": "3fc7b87e-fdc8-4bf0-92e6-96f496a0a717"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nMétriques finales sur l'ensemble de test:\naccuracy: 0.7550\nauc: 0.8543\nprecision: 0.8148\nrecall: 0.6600\nspecificity: 0.8500\nf1: 0.7293\nmcc: 0.5195\nkappa: 0.5100\nlog_loss: 0.6118\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}