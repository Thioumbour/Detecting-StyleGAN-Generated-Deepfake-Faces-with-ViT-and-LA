{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 924245,
          "sourceType": "datasetVersion",
          "datasetId": 464091
        },
        {
          "sourceId": 939937,
          "sourceType": "datasetVersion",
          "datasetId": 501529
        },
        {
          "sourceId": 10102015,
          "sourceType": "datasetVersion",
          "datasetId": 6230878
        },
        {
          "sourceId": 10119502,
          "sourceType": "datasetVersion",
          "datasetId": 6243942
        },
        {
          "sourceId": 10124102,
          "sourceType": "datasetVersion",
          "datasetId": 6247412
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Xtransfer",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "t2bkiD6QnWbm"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "dagnelies_deepfake_faces_path = kagglehub.dataset_download('dagnelies/deepfake-faces')\n",
        "xhlulu_140k_real_and_fake_faces_path = kagglehub.dataset_download('xhlulu/140k-real-and-fake-faces')\n",
        "sokhnaballytour_test_progan_path = kagglehub.dataset_download('sokhnaballytour/test-progan')\n",
        "sokhnaballytour_val_dataset_path = kagglehub.dataset_download('sokhnaballytour/val-dataset')\n",
        "sokhnaballytour_train_dataset_path = kagglehub.dataset_download('sokhnaballytour/train-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Q0CAPaksnWbo"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision timm\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T00:41:19.797716Z",
          "iopub.execute_input": "2025-04-12T00:41:19.79853Z",
          "iopub.status.idle": "2025-04-12T00:42:38.029788Z",
          "shell.execute_reply.started": "2025-04-12T00:41:19.798497Z",
          "shell.execute_reply": "2025-04-12T00:42:38.028758Z"
        },
        "id": "D_JHdHu5nWbo",
        "outputId": "1789dc5a-c078-49d9-cd05-65f693644f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.14)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import (roc_auc_score, accuracy_score, f1_score, precision_score,\n",
        "                             recall_score, matthews_corrcoef, cohen_kappa_score, log_loss, confusion_matrix)\n",
        "import timm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T17:08:07.294205Z",
          "iopub.execute_input": "2025-04-14T17:08:07.294853Z",
          "iopub.status.idle": "2025-04-14T17:08:20.025989Z",
          "shell.execute_reply.started": "2025-04-14T17:08:07.294826Z",
          "shell.execute_reply": "2025-04-14T17:08:20.025373Z"
        },
        "id": "WlHZo2Q5nWbp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 1. Backbone Split: RepVGG-A0 (split into blocks)\n",
        "# -----------------------------\n",
        "def get_repvgg_blocks():\n",
        "    base = timm.create_model('repvgg_a0', pretrained=True)\n",
        "    stages = list(base.stages.children())\n",
        "    blocks = [\n",
        "        base.stem,\n",
        "        stages[0],\n",
        "        stages[1],\n",
        "        stages[2],\n",
        "        stages[3],\n",
        "        nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(1)\n",
        "        )\n",
        "    ]\n",
        "    classifier = nn.Linear(1280, 1)\n",
        "    return nn.ModuleList(blocks), classifier\n",
        "\n",
        "# -----------------------------\n",
        "# 2. X-Transfer Architecture (Block Alternation)\n",
        "# -----------------------------\n",
        "class XTransfer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.master_blocks, self.master_head = get_repvgg_blocks()\n",
        "        self.aux_blocks, _ = get_repvgg_blocks()\n",
        "\n",
        "    def route_blocks(self, x, start='aux'):\n",
        "        out = x\n",
        "        for i in range(len(self.master_blocks)):\n",
        "            if i % 2 == 0:\n",
        "                out = self.aux_blocks[i](out) if start == 'aux' else self.master_blocks[i](out)\n",
        "            else:\n",
        "                out = self.master_blocks[i](out) if start == 'aux' else self.aux_blocks[i](out)\n",
        "        out = self.master_head(out)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "    def forward_master(self, x):\n",
        "        out = x\n",
        "        for block in self.master_blocks:\n",
        "            out = block(out)\n",
        "        out = self.master_head(out)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. AUC Loss (WMW approximation)\n",
        "# -----------------------------\n",
        "def auc_loss(y_true, y_score, gamma=0.16, p=2.0):\n",
        "    y_true = y_true.view(-1)\n",
        "    y_score = y_score.view(-1)\n",
        "    pos = y_score[y_true == 1]\n",
        "    neg = y_score[y_true == 0]\n",
        "    if len(pos) == 0 or len(neg) == 0:\n",
        "        return torch.tensor(0.0, device=y_score.device)\n",
        "    diffs = pos.view(-1, 1) - neg.view(1, -1)\n",
        "    losses = torch.pow(torch.clamp(gamma - diffs, min=0), p)\n",
        "    return losses.mean()\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Loss function\n",
        "# -----------------------------\n",
        "def compute_loss(model, x, y, beta=0.6, s=0.01):\n",
        "    out1 = model.route_blocks(x, start='aux')\n",
        "    out2 = model.route_blocks(x, start='master')\n",
        "    out3 = model.forward_master(x)\n",
        "\n",
        "    BCE = nn.BCELoss()\n",
        "    L1 = BCE(out1, y)\n",
        "    L2 = BCE(out2, y)\n",
        "    L3 = BCE(out3, y)\n",
        "    LAUC = auc_loss(y, out3)\n",
        "\n",
        "    alpha = 2 * L3.item() / (L1.item() + L2.item() + 1e-8)\n",
        "    reg = torch.norm(model.master_head.weight, 2)\n",
        "\n",
        "    loss = alpha * (L1 + L2) + beta * L3 + (1 - beta) * LAUC + s * reg\n",
        "    return loss\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Transforms & Loaders\n",
        "# -----------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
        "    transforms.GaussianBlur(3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "def load_separated_source(train_path, val_path, test_path, batch_size=32):\n",
        "    train_dataset = datasets.ImageFolder(train_path, transform=train_transform)\n",
        "    val_dataset = datasets.ImageFolder(val_path, transform=test_transform)\n",
        "    test_dataset = datasets.ImageFolder(test_path, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def load_data(domain_dir, batch_size=32):\n",
        "    train_dataset = datasets.ImageFolder(os.path.join(domain_dir, 'train'), transform=train_transform)\n",
        "    valid_dataset = datasets.ImageFolder(os.path.join(domain_dir, 'valid'), transform=test_transform)\n",
        "    test_dataset = datasets.ImageFolder(os.path.join(domain_dir, 'test'), transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Phase 1 - Train on Source Domain\n",
        "# -----------------------------\n",
        "def train_on_source(model, train_loader, valid_loader, device, num_epochs=10, lr=0.002):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.001)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        loop = tqdm(train_loader, desc=f\"[Source] Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for x, y in loop:\n",
        "            x, y = x.to(device), y.to(device).float().unsqueeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            out = model.forward_master(x)\n",
        "            loss = F.binary_cross_entropy(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            correct += (out.round().cpu() == y.cpu()).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Phase 2 - Transfer to Target Domain with X-Transfer\n",
        "# -----------------------------\n",
        "def transfer_to_target(model, train_loader, valid_loader, device, num_epochs=10):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.001)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        loop = tqdm(train_loader, desc=f\"[Transfer] Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for x, y in loop:\n",
        "            x, y = x.to(device), y.to(device).float().unsqueeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            loss = compute_loss(model, x, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            out = model.forward_master(x)\n",
        "            correct += (out.round().cpu() == y.cpu()).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Transfer Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Evaluation finale\n",
        "# -----------------------------\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred_probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test = x_test.to(device)\n",
        "            preds = model.forward_master(x_test).cpu().numpy()\n",
        "            y_true.extend(y_test.numpy())\n",
        "            y_pred_probs.extend(preds)\n",
        "\n",
        "    y_pred = [1 if p > 0.5 else 0 for p in y_pred_probs]\n",
        "\n",
        "    auc = roc_auc_score(y_true, y_pred_probs)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    logloss = log_loss(y_true, y_pred_probs)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(f\"MCC: {mcc:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(f\"Specificity: {specificity:.4f}\")\n",
        "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
        "    print(f\"Log Loss: {logloss:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T17:09:01.657167Z",
          "iopub.execute_input": "2025-04-14T17:09:01.65773Z",
          "iopub.status.idle": "2025-04-14T17:09:01.66763Z",
          "shell.execute_reply.started": "2025-04-14T17:09:01.657704Z",
          "shell.execute_reply": "2025-04-14T17:09:01.666849Z"
        },
        "id": "gaxbKqtPnWbq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5. Transforms & Loaders\n",
        "# -----------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
        "    transforms.GaussianBlur(3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "def load_data(train_dir, valid_dir, test_dir, batch_size=32):\n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "    valid_dataset = datasets.ImageFolder(valid_dir, transform=test_transform)\n",
        "    test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Training Pipeline\n",
        "# -----------------------------\n",
        "def train_xtransfer(model, train_loader, valid_loader, device, num_epochs=10, lr=0.002, beta=0.6, s=0.01):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.001)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for x, y in loop:\n",
        "            x, y = x.to(device), y.to(device).float().unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = compute_loss(model, x, y, beta=beta, s=s)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            preds = model.forward_master(x).round()\n",
        "            correct += (preds.cpu() == y.cpu()).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        train_acc = correct / total\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        y_true, y_pred = [], []\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in valid_loader:\n",
        "                x_val, y_val = x_val.to(device), y_val.to(device).float().unsqueeze(1)\n",
        "                outputs = model.forward_master(x_val)\n",
        "                loss = F.binary_cross_entropy(outputs, y_val)\n",
        "                val_loss += loss.item()\n",
        "                preds = outputs.round()\n",
        "                val_correct += (preds.cpu() == y_val.cpu()).sum().item()\n",
        "                val_total += y_val.size(0)\n",
        "                y_true.extend(y_val.cpu().numpy())\n",
        "                y_pred.extend(outputs.cpu().numpy())\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        val_auc = roc_auc_score(y_true, y_pred)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Train Accuracy: {train_acc:.4f}, \"\n",
        "              f\"Valid Loss: {val_loss/len(valid_loader):.4f}, Valid Accuracy: {val_acc:.4f}, Valid ROC-AUC: {val_auc:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Final Evaluation\n",
        "# -----------------------------\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred_probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_loader:\n",
        "            x_test = x_test.to(device)\n",
        "            preds = model.forward_master(x_test).cpu().numpy()\n",
        "            y_true.extend(y_test.numpy())\n",
        "            y_pred_probs.extend(preds)\n",
        "\n",
        "    y_pred = [1 if p > 0.5 else 0 for p in y_pred_probs]\n",
        "\n",
        "    auc = roc_auc_score(y_true, y_pred_probs)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    logloss = log_loss(y_true, y_pred_probs)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(f\"MCC: {mcc:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(f\"Specificity: {specificity:.4f}\")\n",
        "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
        "    print(f\"Log Loss: {logloss:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T17:09:06.365771Z",
          "iopub.execute_input": "2025-04-14T17:09:06.366244Z",
          "iopub.status.idle": "2025-04-14T17:09:06.381121Z",
          "shell.execute_reply.started": "2025-04-14T17:09:06.366212Z",
          "shell.execute_reply": "2025-04-14T17:09:06.38046Z"
        },
        "id": "kiN3r_kunWbr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 8. Exécution\n",
        "# -----------------------------\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_dir = '/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/train'\n",
        "valid_dir = '/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/valid'\n",
        "test_dir  = '/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'\n",
        "\n",
        "train_loader, valid_loader, test_loader = load_data(train_dir, valid_dir, test_dir)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T09:45:46.661253Z",
          "iopub.execute_input": "2025-04-14T09:45:46.662048Z",
          "iopub.status.idle": "2025-04-14T09:48:09.15118Z",
          "shell.execute_reply.started": "2025-04-14T09:45:46.662018Z",
          "shell.execute_reply": "2025-04-14T09:48:09.150623Z"
        },
        "id": "PcCf-4a9nWbt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = XTransfer()\n",
        "train_xtransfer(model, train_loader, valid_loader, device, num_epochs=10)\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-12T14:19:53.353098Z",
          "iopub.execute_input": "2025-04-12T14:19:53.353296Z",
          "iopub.status.idle": "2025-04-12T21:20:26.776511Z",
          "shell.execute_reply.started": "2025-04-12T14:19:53.353276Z",
          "shell.execute_reply": "2025-04-12T21:20:26.775835Z"
        },
        "id": "ocmR6pxhnWbt",
        "outputId": "788abdfa-e541-4029-d95b-81ea0d3c425b",
        "colab": {
          "referenced_widgets": [
            "3b058a35e67945c484439ea3a56a7243"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/36.6M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b058a35e67945c484439ea3a56a7243"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Epoch 1/10: 100%|██████████| 3125/3125 [49:51<00:00,  1.04it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10, Train Loss: 1.2399, Train Accuracy: 0.8559, Valid Loss: 0.4247, Valid Accuracy: 0.8030, Valid ROC-AUC: 0.9367\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/10: 100%|██████████| 3125/3125 [39:44<00:00,  1.31it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/10, Train Loss: 0.8477, Train Accuracy: 0.9436, Valid Loss: 0.6320, Valid Accuracy: 0.7345, Valid ROC-AUC: 0.9609\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/10: 100%|██████████| 3125/3125 [39:32<00:00,  1.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/10, Train Loss: 0.6867, Train Accuracy: 0.9622, Valid Loss: 0.3394, Valid Accuracy: 0.8591, Valid ROC-AUC: 0.9721\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/10: 100%|██████████| 3125/3125 [39:19<00:00,  1.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4/10, Train Loss: 0.5992, Train Accuracy: 0.9662, Valid Loss: 0.2688, Valid Accuracy: 0.8918, Valid ROC-AUC: 0.9702\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/10: 100%|██████████| 3125/3125 [38:35<00:00,  1.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5/10, Train Loss: 0.5386, Train Accuracy: 0.9664, Valid Loss: 0.5487, Valid Accuracy: 0.7820, Valid ROC-AUC: 0.9791\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/10: 100%|██████████| 3125/3125 [38:12<00:00,  1.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6/10, Train Loss: 0.4960, Train Accuracy: 0.9625, Valid Loss: 0.3977, Valid Accuracy: 0.8435, Valid ROC-AUC: 0.9798\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/10: 100%|██████████| 3125/3125 [37:49<00:00,  1.38it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7/10, Train Loss: 0.4632, Train Accuracy: 0.9561, Valid Loss: 0.8283, Valid Accuracy: 0.7220, Valid ROC-AUC: 0.9806\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/10: 100%|██████████| 3125/3125 [37:53<00:00,  1.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8/10, Train Loss: 0.4443, Train Accuracy: 0.9483, Valid Loss: 0.4266, Valid Accuracy: 0.8337, Valid ROC-AUC: 0.9829\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/10: 100%|██████████| 3125/3125 [37:52<00:00,  1.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9/10, Train Loss: 0.4366, Train Accuracy: 0.9422, Valid Loss: 0.5105, Valid Accuracy: 0.8105, Valid ROC-AUC: 0.9830\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/10: 100%|██████████| 3125/3125 [37:51<00:00,  1.38it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10/10, Train Loss: 0.4239, Train Accuracy: 0.9394, Valid Loss: 0.3889, Valid Accuracy: 0.8472, Valid ROC-AUC: 0.9831\nAccuracy: 0.8545\nAUC: 0.9828\nMCC: 0.7371\nF1-score: 0.8720\nSpecificity: 0.7175\nCohen's Kappa: 0.7089\nLog Loss: 0.3784\nRecall: 0.9914\nPrecision: 0.7782\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wang"
      ],
      "metadata": {
        "id": "43ZaphbrnWbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 8. Exécution\n",
        "# -----------------------------\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_dir = '/kaggle/input/train-dataset/train_dataset'\n",
        "valid_dir = '/kaggle/input/val-dataset/val_dataset'\n",
        "test_dir  = '/kaggle/input/test-progan/progan/person'\n",
        "\n",
        "train_loader, valid_loader, test_loader = load_data(train_dir, valid_dir, test_dir)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T09:38:10.1674Z",
          "iopub.execute_input": "2025-04-14T09:38:10.168203Z",
          "iopub.status.idle": "2025-04-14T09:38:43.736935Z",
          "shell.execute_reply.started": "2025-04-14T09:38:10.168165Z",
          "shell.execute_reply": "2025-04-14T09:38:43.736053Z"
        },
        "id": "j2GOaqgWnWbw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "train_dir = '/kaggle/input/train-dataset/train_dataset'\n",
        "valid_dir = '/kaggle/input/val-dataset/val_dataset'\n",
        "test_dir  = '/kaggle/input/test-progan/progan/person'\n",
        "\n",
        "def load_data(train_dir, valid_dir, test_dir, batch_size=32, train_ratio=1.0, seed=42):\n",
        "    # Chargement complet des datasets\n",
        "    train_dataset_full = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "    valid_dataset = datasets.ImageFolder(valid_dir, transform=test_transform)\n",
        "    test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "    if train_ratio < 1.0:\n",
        "        # Organiser les indices par classe\n",
        "        class_indices = defaultdict(list)\n",
        "        for idx, (_, label) in enumerate(train_dataset_full.samples):\n",
        "            class_indices[label].append(idx)\n",
        "\n",
        "        # Équilibrage et sous-échantillonnage\n",
        "        np.random.seed(seed)\n",
        "        selected_indices = []\n",
        "\n",
        "        for label, indices in class_indices.items():\n",
        "            np.random.shuffle(indices)\n",
        "            subset_size = int(len(indices) * train_ratio)\n",
        "            selected_indices.extend(indices[:subset_size])\n",
        "\n",
        "        # Créer le subset équilibré\n",
        "        train_dataset = Subset(train_dataset_full, selected_indices)\n",
        "    else:\n",
        "        train_dataset = train_dataset_full\n",
        "\n",
        "    # Loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "train_loader, valid_loader, test_loader = load_data(\n",
        "    train_dir,\n",
        "    valid_dir,\n",
        "    test_dir,\n",
        "    batch_size=32,\n",
        "    train_ratio=0.4  # Utilise 30% du dataset d'entraînement\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T17:20:04.982437Z",
          "iopub.execute_input": "2025-04-14T17:20:04.983347Z",
          "iopub.status.idle": "2025-04-14T17:23:43.05444Z",
          "shell.execute_reply.started": "2025-04-14T17:20:04.983307Z",
          "shell.execute_reply": "2025-04-14T17:23:43.053658Z"
        },
        "id": "6QuRIXpbnWbx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = XTransfer()\n",
        "train_xtransfer(model, train_loader, valid_loader, device, num_epochs=10)\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T10:00:04.814712Z",
          "iopub.execute_input": "2025-04-14T10:00:04.815284Z",
          "iopub.status.idle": "2025-04-14T15:29:44.857458Z",
          "shell.execute_reply.started": "2025-04-14T10:00:04.815254Z",
          "shell.execute_reply": "2025-04-14T15:29:44.856679Z"
        },
        "id": "UUR0d7wunWbx",
        "outputId": "a38e6b32-d2c5-40b2-e361-44dcad0b7e83"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1/10: 100%|██████████| 2251/2251 [39:28<00:00,  1.05s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10, Train Loss: 1.3261, Train Accuracy: 0.8463, Valid Loss: 0.4086, Valid Accuracy: 0.8025, Valid ROC-AUC: 0.9286\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/10: 100%|██████████| 2251/2251 [32:01<00:00,  1.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/10, Train Loss: 0.8028, Train Accuracy: 0.9549, Valid Loss: 1.2478, Valid Accuracy: 0.6525, Valid ROC-AUC: 0.9063\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/10: 100%|██████████| 2251/2251 [31:55<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/10, Train Loss: 0.5832, Train Accuracy: 0.9758, Valid Loss: 0.6789, Valid Accuracy: 0.7535, Valid ROC-AUC: 0.9607\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/10: 100%|██████████| 2251/2251 [31:50<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4/10, Train Loss: 0.4704, Train Accuracy: 0.9814, Valid Loss: 0.3143, Valid Accuracy: 0.8765, Valid ROC-AUC: 0.9765\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/10: 100%|██████████| 2251/2251 [31:50<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5/10, Train Loss: 0.3907, Train Accuracy: 0.9835, Valid Loss: 0.4955, Valid Accuracy: 0.8365, Valid ROC-AUC: 0.9779\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/10: 100%|██████████| 2251/2251 [31:50<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6/10, Train Loss: 0.3539, Train Accuracy: 0.9805, Valid Loss: 0.4497, Valid Accuracy: 0.8525, Valid ROC-AUC: 0.9764\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/10: 100%|██████████| 2251/2251 [32:00<00:00,  1.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7/10, Train Loss: 0.3145, Train Accuracy: 0.9763, Valid Loss: 0.6192, Valid Accuracy: 0.8150, Valid ROC-AUC: 0.9792\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/10: 100%|██████████| 2251/2251 [32:02<00:00,  1.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8/10, Train Loss: 0.2934, Train Accuracy: 0.9715, Valid Loss: 0.4382, Valid Accuracy: 0.8565, Valid ROC-AUC: 0.9823\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/10: 100%|██████████| 2251/2251 [32:08<00:00,  1.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9/10, Train Loss: 0.2905, Train Accuracy: 0.9644, Valid Loss: 0.7989, Valid Accuracy: 0.7785, Valid ROC-AUC: 0.9748\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/10: 100%|██████████| 2251/2251 [32:01<00:00,  1.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10/10, Train Loss: 0.2752, Train Accuracy: 0.9619, Valid Loss: 0.5398, Valid Accuracy: 0.8270, Valid ROC-AUC: 0.9804\nAccuracy: 0.7750\nAUC: 0.9628\nMCC: 0.6092\nF1-score: 0.7134\nSpecificity: 0.9900\nCohen's Kappa: 0.5500\nLog Loss: 0.8366\nRecall: 0.5600\nPrecision: 0.9825\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deepfake Faces"
      ],
      "metadata": {
        "id": "KDVi2M2PnWbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import iplot\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def get_data():\n",
        "    return pd.read_csv('/kaggle/input/deepfake-faces/metadata.csv')\n",
        "\n",
        "meta=get_data()\n",
        "\n",
        "real_df = meta[meta[\"label\"] == \"REAL\"]\n",
        "fake_df = meta[meta[\"label\"] == \"FAKE\"]\n",
        "sample_size = 16293\n",
        "\n",
        "real_df = real_df.sample(sample_size, random_state=42)\n",
        "fake_df = fake_df.sample(sample_size, random_state=42)\n",
        "\n",
        "sample_meta = pd.concat([real_df, fake_df])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Train_set, Test_set = train_test_split(sample_meta,test_size=0.2,random_state=42,stratify=sample_meta['label'])\n",
        "Train_set, Val_set  = train_test_split(Train_set,test_size=0.3,random_state=42,stratify=Train_set['label'])\n",
        "\n",
        "def retreive_dataset(set_name):\n",
        "    images,labels=[],[]\n",
        "    for (img, imclass) in zip(set_name['videoname'], set_name['label']):\n",
        "        # Construct the image path using os.path.join\n",
        "        image_path = os.path.join('/kaggle/input/deepfake-faces/faces_224', img[:-4] + '.jpg')\n",
        "\n",
        "        # Check if the image file exists before attempting to load it\n",
        "        if os.path.exists(image_path):\n",
        "            image = cv2.imread(image_path)\n",
        "\n",
        "            # Check if the image was loaded successfully\n",
        "            if image is not None:\n",
        "                images.append(image)\n",
        "                if(imclass=='FAKE'):\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    labels.append(0)\n",
        "            else:\n",
        "                print(f\"Warning: Could not load image at path: {image_path}\")\n",
        "        else:\n",
        "            print(f\"Warning: Image file does not exist at path: {image_path}\")\n",
        "\n",
        "    return np.array(images),np.array(labels)\n",
        "\n",
        "X_train,y_train=retreive_dataset(Train_set)\n",
        "X_val,y_val=retreive_dataset(Val_set)\n",
        "X_test,y_test=retreive_dataset(Test_set)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T17:09:16.891947Z",
          "iopub.execute_input": "2025-04-14T17:09:16.89222Z",
          "iopub.status.idle": "2025-04-14T17:14:31.817366Z",
          "shell.execute_reply.started": "2025-04-14T17:09:16.892201Z",
          "shell.execute_reply": "2025-04-14T17:14:31.816767Z"
        },
        "id": "t4YDVDPJnWby",
        "outputId": "cffa6502-20cf-4aab-e6cf-34b6dffeac0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-14 17:09:18.720944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744650558.963690      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744650559.041934      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Dataset personnalisé pour les images de deepfake\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        # Conversion BGR -> RGB car OpenCV charge en BGR mais PyTorch attend RGB\n",
        "        self.images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images]\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Créer les dataloaders\n",
        "train_dataset = DeepfakeDataset(X_train, y_train, transform=train_transform)\n",
        "val_dataset = DeepfakeDataset(X_val, y_val, transform=test_transform)\n",
        "test_dataset = DeepfakeDataset(X_test, y_test, transform=test_transform)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T17:19:10.30043Z",
          "iopub.execute_input": "2025-04-14T17:19:10.301114Z",
          "iopub.status.idle": "2025-04-14T17:19:13.587066Z",
          "shell.execute_reply.started": "2025-04-14T17:19:10.301089Z",
          "shell.execute_reply": "2025-04-14T17:19:13.586234Z"
        },
        "id": "2PChf7CpnWby"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = XTransfer()\n",
        "train_xtransfer(model, train_loader, valid_loader, device, num_epochs=10)\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-14T15:38:17.150487Z",
          "iopub.execute_input": "2025-04-14T15:38:17.151234Z",
          "iopub.status.idle": "2025-04-14T16:18:27.024274Z",
          "shell.execute_reply.started": "2025-04-14T15:38:17.15121Z",
          "shell.execute_reply": "2025-04-14T16:18:27.023313Z"
        },
        "id": "W9Ho9ijTnWby",
        "outputId": "5bd870a2-87b2-4378-c75c-6700a30be50d"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1/10: 100%|██████████| 570/570 [03:51<00:00,  2.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10, Train Loss: 1.6318, Train Accuracy: 0.7475, Valid Loss: 0.5663, Valid Accuracy: 0.7144, Valid ROC-AUC: 0.7808\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/10: 100%|██████████| 570/570 [03:50<00:00,  2.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/10, Train Loss: 1.4079, Train Accuracy: 0.8235, Valid Loss: 0.5107, Valid Accuracy: 0.7503, Valid ROC-AUC: 0.8282\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/10: 100%|██████████| 570/570 [03:50<00:00,  2.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/10, Train Loss: 1.2110, Train Accuracy: 0.8739, Valid Loss: 0.4909, Valid Accuracy: 0.7635, Valid ROC-AUC: 0.8465\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/10: 100%|██████████| 570/570 [03:51<00:00,  2.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4/10, Train Loss: 1.0304, Train Accuracy: 0.9104, Valid Loss: 0.4658, Valid Accuracy: 0.7794, Valid ROC-AUC: 0.8626\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/10: 100%|██████████| 570/570 [03:49<00:00,  2.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5/10, Train Loss: 0.8794, Train Accuracy: 0.9275, Valid Loss: 0.4629, Valid Accuracy: 0.7807, Valid ROC-AUC: 0.8674\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/10: 100%|██████████| 570/570 [03:49<00:00,  2.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6/10, Train Loss: 0.7444, Train Accuracy: 0.9388, Valid Loss: 0.4796, Valid Accuracy: 0.7842, Valid ROC-AUC: 0.8705\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/10: 100%|██████████| 570/570 [03:50<00:00,  2.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7/10, Train Loss: 0.6451, Train Accuracy: 0.9441, Valid Loss: 0.4655, Valid Accuracy: 0.7941, Valid ROC-AUC: 0.8770\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/10: 100%|██████████| 570/570 [03:51<00:00,  2.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8/10, Train Loss: 0.5674, Train Accuracy: 0.9429, Valid Loss: 0.4678, Valid Accuracy: 0.7976, Valid ROC-AUC: 0.8771\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/10: 100%|██████████| 570/570 [03:51<00:00,  2.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9/10, Train Loss: 0.5294, Train Accuracy: 0.9390, Valid Loss: 0.4758, Valid Accuracy: 0.7934, Valid ROC-AUC: 0.8768\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/10: 100%|██████████| 570/570 [03:51<00:00,  2.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10/10, Train Loss: 0.5037, Train Accuracy: 0.9371, Valid Loss: 0.4764, Valid Accuracy: 0.7944, Valid ROC-AUC: 0.8769\nAccuracy: 0.7959\nAUC: 0.8824\nMCC: 0.5924\nF1-score: 0.8001\nSpecificity: 0.7754\nCohen's Kappa: 0.5919\nLog Loss: 0.4652\nRecall: 0.8165\nPrecision: 0.7843\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}